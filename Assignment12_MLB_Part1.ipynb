{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hybrid-salad",
   "metadata": {},
   "source": [
    "Michael Beauchamp\n",
    "Assignment 12\n",
    "Data 620 Section 9040\n",
    "Summer 2021\n",
    "Prof. Carrie Beam\n",
    "\n",
    "This part of the code takes the original files that contain the first State of the Union addresses for various presidents through history and returns files of all the words in the speech and the number of times they occur.  The second file in the code takes those files  in to one csv by part one and combines them into 1 file.  Then, that final file is converted to a Excel file for processing in Tableau.\n",
    "\n",
    "Load all of the original speech files into the Binder and run this code for each of those.  You get a Final file for each speech.  Then run code part 2 to combine all of the files into 1 and conver to xlsx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "electronic-nation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /srv/conda/envs/notebook/lib/python3.6/site-packages (3.6.2)\n",
      "Requirement already satisfied: tqdm in /srv/conda/envs/notebook/lib/python3.6/site-packages (from nltk) (4.62.0)\n",
      "Requirement already satisfied: regex in /srv/conda/envs/notebook/lib/python3.6/site-packages (from nltk) (2021.8.3)\n",
      "Requirement already satisfied: joblib in /srv/conda/envs/notebook/lib/python3.6/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: click in /srv/conda/envs/notebook/lib/python3.6/site-packages (from nltk) (8.0.1)\n",
      "Requirement already satisfied: importlib-metadata in /srv/conda/envs/notebook/lib/python3.6/site-packages (from click->nltk) (3.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from importlib-metadata->click->nltk) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from importlib-metadata->click->nltk) (3.7.4.3)\n",
      "Requirement already satisfied: matplotlib in /srv/conda/envs/notebook/lib/python3.6/site-packages (3.3.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /srv/conda/envs/notebook/lib/python3.6/site-packages/cycler-0.10.0-py3.6.egg (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from matplotlib) (8.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: numpy>=1.15 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from matplotlib) (1.19.5)\n",
      "Requirement already satisfied: six in /srv/conda/envs/notebook/lib/python3.6/site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Requirement already satisfied: openpyxl in /srv/conda/envs/notebook/lib/python3.6/site-packages (3.0.7)\n",
      "Requirement already satisfied: et-xmlfile in /srv/conda/envs/notebook/lib/python3.6/site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: XlsxWriter in /srv/conda/envs/notebook/lib/python3.6/site-packages (1.4.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here we install the nltk package onto the Try Jupyter environment. This needs to be run every time you load in the code.\n",
    "#Import necessary libraries\n",
    "import sys\n",
    "!{sys.executable} -m pip install nltk\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "!{sys.executable} -m pip install openpyxl\n",
    "!{sys.executable} -m pip install XlsxWriter\n",
    "import pandas as pd\n",
    "import csv\n",
    "import string\n",
    "import nltk #This is a bit redundant, but necessary for the Try Jupyter environment.\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "advanced-valley",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we load in the file based on the filename.\n",
    "#If the filename doesn't exist, the program exits.\n",
    "\n",
    "#change file name for each of the .txt files of the state of the union addresses\n",
    "#Example, change Biden2021 to Clinton1993 to process the speech for Clinton\n",
    "filename = 'Biden2021.txt'\n",
    "\n",
    "try:\n",
    "    file = open(filename)\n",
    "except:\n",
    "    print('File cannot be opened: ', filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "metric-genealogy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we load in the stopwords from the downloaded nltk stopwords list.\n",
    "#Added the snowball stemmer\n",
    "snowball = SnowballStemmer(language = 'english')\n",
    "lmtzr = WordNetLemmatizer()\n",
    "stop = stopwords.words('english')\n",
    "#appendng words to the stop list\n",
    "stop.append(\"—\")\n",
    "stop.append(\"let’s\")\n",
    "stop.append(\"that’s\")\n",
    "stop.append(\"it’s\")\n",
    "stop.append(\"we’re\")\n",
    "stop.append(\"can’t\")\n",
    "stop.append(\"I’ve\")\n",
    "stop.append(\"I’m\")\n",
    "stop.append(\"Don’t\")\n",
    "stop.append(\"there’s\")\n",
    "stop.append(\"american\")\n",
    "stop.append(\"americans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "another-wrist",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort and reverse the list\n",
    "def sort_and_reverse(lst):\n",
    "    lst.sort(reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "welcome-keeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes in the file, stopwords list, and number of important words and returns the most frequent words.\n",
    "def most_frequent_words(file, stop, N):  \n",
    "    #Here we create a dictionary, which we will assign a value to each word corresponding to the number of times this word appears.\n",
    "    counts = dict()\n",
    "    #go throught the file and seperate the words\n",
    "    for line in file:\n",
    "        line = line.rstrip()\n",
    "        line = line.translate(line.maketrans('', '', string.punctuation))\n",
    "        line = line.lower()\n",
    "        words = line.split()\n",
    "\n",
    "        #Now processes the words list, and removes the stop words.\n",
    "        for word in words:\n",
    "            if word not in stop:\n",
    "                #Lemmatizing- we group similar word inflections together (e.g. rock and rocks becomes the same word)\n",
    "                lmtzr.lemmatize(word)\n",
    "                #added the snowball stemmer becuase I was getting America, American and Americans\n",
    "                #this didn't work so added American and Americans to stop list\n",
    "                snowball.stem(word)\n",
    "                if word not in counts:\n",
    "                    counts[word] = 1\n",
    "                else:\n",
    "                    counts[word] += 1\n",
    "\n",
    "    #Puts the dictionary of counts into a list and adds a column with the president's name.\n",
    "    #Makes it easier to work in tableau, must change President name/year with each file\n",
    "    #Example, change Biden to Clinton and 2021 to 1993. File name also has year in it.\n",
    "    word_list = [(counts[w], w, \"Biden\", \"2021\") for w in counts]\n",
    "    \n",
    "    #Sort and Reverse the list\n",
    "    sort_and_reverse(word_list)\n",
    "\n",
    "    #Write the list to a csv file called Final*.csv where * is the president's name\n",
    "    #Must change president name with each new file\n",
    "    with open('FinalBiden.csv', 'w') as a:\n",
    "        writer = csv.writer(a)\n",
    "        #add the column names\n",
    "        writer.writerow(['Count', 'Word','President','Year'])\n",
    "        writer.writerows(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fifth-suspension",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This fuction currently returns just the top 1 word. \n",
    "#You can fix the arguments you pass in to change the number of words this returns.\n",
    "most_frequent_words(file, stop, 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
